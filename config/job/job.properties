### kafka
kafka.bootstrap.servers=zqykjtp02:9092,zqykjtp03:9092,zqykjtp04:9092
kafka.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.value.deserializer=org.apache.kafka.common.serialization.ByteArrayDeserializer
# earliest latest
kafka.offset.reset=earliest
kafka.enable.auto.commit=false
kafka.consume.group.id=consumer.processor.group.id
kafka.topic.partitions=1
# iod_gcjlout foshan_test
kafka.topic.name=nhga
kafka.max.partition.fetch.bytes=10485760
kafka.max.poll.records=10000
kafka.fetch.interval.millisecond=1000

# persist batch size to solr
solr.batch.size=20000
solr.entity.collection=global_foshan_standard_model_entity_index
solr.relation.collection=global_foshan_standard_model_relation_index

# from kafka
elp.dbmap.ds=kafka
elp.dbmap.elp.model.id=foshan_standard_model
# bayonet_pass_record
elp.dbmap.tablename.bpr=bayonet_pass_record
elp.dbmap.elp.type.bpr=bayonet_pass_record
# vehicle
elp.dbmap.tablename.vehicle=bayonet_pass_record
elp.dbmap.elp.type.vehicle=vehicle

### mongo
job.mongo.hostname=172.30.6.34
job.mongo.port=27017
job.mongo.database=hyjj

### clear data of cron expression

kafka.consume.backup.group.id=consumerBackupGroupId
### backup:消息线程读取数据，存储的本地路径
backup.read.local.path=kafka/backup/read
#backup:缓存数据超过该值时,则落地缓存数据
backup.read.max.size=1M
#backup:缓存数据超时时,则落地缓存数据
backup.read.max.time=60
#backup:把本地文件放到hdfs上的路径,相对路径
backup.move.path=backup/read
#backup:合并hdfs上文件的存储路径,相对路径
backup.merge.path=backup/compact
#backup:超过该值时,则合并文件
backup.merge.max.size=1M
